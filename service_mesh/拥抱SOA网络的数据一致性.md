https://blog.envoyproxy.io/embracing-eventual-consistency-in-soa-networking-32a5ee5d443d

Envoy的基本设计原则之一是最终的一致性，几乎渗透到系统的每个方面，从线程模型到服务和配置发现（参见这里和这里以获得更多信息）。

当强若最终一致性的范式应用于SoA网络时，我注意到在过去的几年中，许多人发现它是违背常规的。 这并不奇怪 - 最终的一致性在理论上是有道理的，但是当实际应用于许多现实世界的问题时则迅速出现混乱。

当代数据库设计就是一个很好的例子，试图平衡最终一致性的性能收益与强一致性对程序员的易用性。 业界已经看到使用趋势从强一致的事务性RDBMS（如MySQL）转向最终一致的NoSQL系统（如DynamoDB），转而回到可扩展的一致性事务性RDBMS（如Spanner）。 关于为什么程序员应该总是选择强一致性，这个帖子是一个很好的阅读这个话题。 （顺便说一句，我认为Cloud Spanner是一个绝对改变游戏规则的技术，但这不是本文的主题。）

在这篇文章中，我将讨论为什么在SoA网络中拥抱最终的一致性是如此重要。 我还将解释一些分布式系统程序员需要重新考虑如何评估和使用最终一致的服务发现和负载平衡系统以获取全部价值的方式。

## 为什么最终一致性

从根本上说，分布式系统的核心是最终一致的：
计算节点随时可能死亡或暂时失去连接。
网络路径可能会死亡或暂时中断，导致路由更改并可能c出现脑裂的情况。
无论是由于滚动部署，金丝雀，cron抖动还是许多其他因素，软件部署都不会同时发生。
自动缩放通过设计来频繁更改系统拓扑。 节点不会同时关闭，也不会同时启动。 随着行业越来越趋向FaaS，这个事实只会变得更加明显。

另请参阅Werner Vogels撰写的关于深入讨论计算机科学的最终一致性的文章。

当然，可以在最终一致的基础之上对强一致性进行分层; Spanner是一个工程奇迹，就是这样做的。 然而，领导选举，法定人数，共识算法等在工程和运营复杂性方面都有成本，更重要的是增加了延迟。 强一致性保证不是免费的。 对Spanner的读写操作时长可能比应用于最终一致的NoSQL系统要多一个数量级。

回到Envoy和基于SoA的网络和配置发现，对于在数万，数十万甚至数百万个节点中的每一个节点有可能具有整系统的一致的视图吗？ 我不会争辩。 然而，历史上，许多分布式网络系统依靠强一致存储（例如，ZooKeeper）来保存成员资格和软件版本数据。 将这些存储扩展到大数据量是困难的并且会经常导致停机。 那为什么要麻烦？

当最初设计Envoy时，我的目标是拥抱核心分布式网络的最终一致性，并将其推向其逻辑极端：

如果成员数据不一致，为什么我们不能在每个发现服务节点的内存中缓存数十秒？
如果在每个发现服务节点中成员数据被缓存了几十秒，为什么每个envoy都不能缓存几十秒呢？
如果每个envoy缓存数据达数十秒，为什么每个工作线程中都需要envoy的一致性？ （有关这方面的更多信息，请参阅Envoy的线程模型的文章。）

答案是，当涉及分布式网络的成员资格和配置时，不需要在任何地方保持一致性，因为在任何地方都不可能有一致性。 拥抱最终的一致性到处都会产生一个更简单的系统，性能更高，更容易操作。

## 一致性和envoy的xDS API
我不会在帖子中重复介绍Envoy API。 相关完整的讲解，请参阅我对通用数据平面API和Envoy动态配置的文档的博文。

我将谈到由于编程的简单性和性能的原因，所有的envoy API都是最终一致的，即使他们可能在逻辑上相关，但不同。 例如，路由发现服务（RDS）API提供的路由可以引用由集群发现服务（CDS）API提供服务的集群。 但RDS并不明确依赖于CDS。 这意味着如果Envoy连接到RDS API和CDS API的不同管理服务器，则可能很容易向Envoy发送一个尚未被CDS API的管理服务器定义的群集的路由更新。

前面的内容正是许多新用户对Envoy配置和发现模型感到困惑的原因。 他们问：如果API是相关的，但最终是一致的和分离的，是不是会导致envoy做错事的情况呢？ 从技术上讲，事实上最终一致的API消费的可能会导致诸如前面所述的路线涉及尚不存在的集群的情况。 但是，这种情况是否真的发生在现实世界？

## 如何在现实世界中应用分布式负载平衡和路由配置？

最近我最感兴趣的事情之一就是观察人们如何评价envoy和Istio这样的系统。 从简单的测试开始“踢轮胎”是人的本性。初始用户可能立即做两件事情：

1.执行某种类型的“赛马”的基准测试，将envoy与HAProxy，NGINX或其他一些负载均衡器进行比较（例如：查看是否可以按每个CPU线程来推送25K + RPS的合成HTTP流量）。
2.与Kubernetes一起使用Istio或为Envoy编写一个简单的xDS服务器，然后使用RDS和CDS执行小型测试来将流量立即从群集A转移到全新的群集B.

我不会花很多时间讨论（1）。 我理解为什么新用户这样做，虽然这让我感到沮丧，因为这些基准在确定真实世界效能方面有多么不切实际（您最后一次在真实系统中每次执行25K RPS每线程琐碎请求和响应的时间？）。 性能是非常重要的，但是真实世界的性能是非常复杂的，并且基于P99的系统功能和实际工作负载的性能。 这些事实主要决定了需要多少容量余量来满足使用突发。

（2）以上更有意思。

图1示出了上述场景（2）的图形描绘。 在这种情况下，RDS提供路由信息，而CDS提供静态集群信息（真正的实现可能还会使用端点发现服务API，但不需要使此示例更复杂）。 假设以下一系列事件：

1.CDS提供了集群A的静态端点定义
2.RDS提供路由映射到集群A.
3.运维人员进行更改，立即让CDS提供集群B，同时删除集群A，并让RDS提供路由映射到集群B.

在使用Envoy最终一致的模式时，在这种情况下会发生什么？ 特使很可能会有一段时间在服务404或503s（取决于配置）。 这是因为系统中的每个组件最终都会收敛。 这意味着，以不确定的顺序，特使最终将删除集群A并将其替换为集群B，并最终将替换从集群A到集群B的路由映射。在收敛的中间阶段，映射可能是无效的，导致错误响应。

这难道不是一个错误？ 大多数用户最初都是这样看待这个观点的。 事实上，由于用户的这种初步反应非常普遍，大多数产品也将其视为一个错误，即使在任何设计良好的分布式系统运行期间，上述情况都不会发生在现实世界中。

在现实世界中，大致发生如下的一系列事件：

